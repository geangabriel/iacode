{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIPSnxCW3roR9E72KIiFT0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geangabriel/iacode/blob/main/Untitled7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow numpy matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdhCPkd2qN12",
        "outputId": "2fca2c0c-7252-435d-9db4-d4796be7f361"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1Rdw0_Gp5Rp",
        "outputId": "66a3f24e-efa4-4df1-b72d-5fa6b77532d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Versão do TensorFlow: 2.18.0\n",
            "\n",
            "Formato dos dados de treino: (60000, 784)\n",
            "Formato dos dados de teste: (10000, 784)\n",
            "\n",
            "--- Arquivo de pesos 'meu_compressor_vae.weights.h5' encontrado! Carregando conhecimento... ---\n",
            "--- Treinando por mais 15 épocas... ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - kl_loss: 6.4702 - loss: 26.5777 - reconstruction_loss: 20.1074 - val_kl_loss: 6.4584 - val_loss: 26.2675 - val_reconstruction_loss: 19.8091\n",
            "Epoch 2/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 19ms/step - kl_loss: 6.4643 - loss: 26.5407 - reconstruction_loss: 20.0765 - val_kl_loss: 6.4012 - val_loss: 26.2524 - val_reconstruction_loss: 19.8512\n",
            "Epoch 3/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 16ms/step - kl_loss: 6.4355 - loss: 26.5067 - reconstruction_loss: 20.0712 - val_kl_loss: 6.4248 - val_loss: 26.2930 - val_reconstruction_loss: 19.8682\n",
            "Epoch 4/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - kl_loss: 6.4727 - loss: 26.6090 - reconstruction_loss: 20.1363 - val_kl_loss: 6.5478 - val_loss: 26.2836 - val_reconstruction_loss: 19.7357\n",
            "Epoch 5/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - kl_loss: 6.4541 - loss: 26.5128 - reconstruction_loss: 20.0588 - val_kl_loss: 6.4109 - val_loss: 26.2785 - val_reconstruction_loss: 19.8676\n",
            "Epoch 6/15\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 18ms/step - kl_loss: 6.4815 - loss: 26.5557 - reconstruction_loss: 20.0742 - val_kl_loss: 6.4976 - val_loss: 26.2977 - val_reconstruction_loss: 19.8002\n",
            "Epoch 7/15\n",
            "\u001b[1m419/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - kl_loss: 6.4408 - loss: 26.4905 - reconstruction_loss: 20.0496"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "### 1. IMPORTAÇÕES E CONFIGURAÇÃO\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os  # <-- ADICIONADO: Para verificar se o arquivo de pesos existe\n",
        "\n",
        "print(\"Versão do TensorFlow:\", tf.__version__)\n",
        "\n",
        "### 2. PARÂMETROS DO MODELO\n",
        "ORIGINAL_DIM = 28 * 28\n",
        "# Aumentei a dimensão latente para 16 para obter resultados visuais melhores.\n",
        "# Com dim=2 a compressão é extrema e a imagem fica muito borrada.\n",
        "# Com dim=16 você verá uma melhoria mais clara ao continuar o treinamento.\n",
        "LATENT_DIM = 16  # <-- MODIFICADO: Aumentado para melhor qualidade\n",
        "EPOCHS = 15      # <-- MODIFICADO: Aumentei um pouco as épocas\n",
        "BATCH_SIZE = 128\n",
        "# Nome do arquivo para salvar o conhecimento do modelo\n",
        "WEIGHTS_FILE = \"meu_compressor_vae.weights.h5\"  # <-- ADICIONADO\n",
        "\n",
        "### 3. PREPARAÇÃO DOS DADOS\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
        "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
        "\n",
        "print(\"\\nFormato dos dados de treino:\", x_train.shape)\n",
        "print(\"Formato dos dados de teste:\", x_test.shape)\n",
        "\n",
        "\n",
        "### 4. CONSTRUÇÃO DO MODELO VAE (Seu código, sem alterações)\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"Usa z_mean e z_log_var para amostrar z, o vetor que representa o dígito.\"\"\"\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(ORIGINAL_DIM,))\n",
        "x = layers.Dense(256, activation='relu')(encoder_inputs)\n",
        "z_mean = layers.Dense(LATENT_DIM, name='z_mean')(x)\n",
        "z_log_var = layers.Dense(LATENT_DIM, name='z_log_var')(x)\n",
        "z = Sampling()([z_mean, z_log_var])\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "\n",
        "latent_inputs = keras.Input(shape=(LATENT_DIM,))\n",
        "x = layers.Dense(256, activation='relu')(latent_inputs)\n",
        "decoder_outputs = layers.Dense(ORIGINAL_DIM, activation='sigmoid')(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name='decoder')\n",
        "\n",
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super(VAE, self).__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstruction = self.decoder(z)\n",
        "        return reconstruction\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var, z = self.encoder(data)\n",
        "            reconstruction = self.decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.binary_crossentropy(data, reconstruction), axis=-1))\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "            total_loss = reconstruction_loss + kl_loss\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\"loss\": self.total_loss_tracker.result(), \"reconstruction_loss\": self.reconstruction_loss_tracker.result(), \"kl_loss\": self.kl_loss_tracker.result()}\n",
        "\n",
        "    def test_step(self, data):\n",
        "        z_mean, z_log_var, z = self.encoder(data)\n",
        "        reconstruction = self.decoder(z)\n",
        "        reconstruction_loss = tf.reduce_mean(tf.reduce_sum(keras.losses.binary_crossentropy(data, reconstruction), axis=-1))\n",
        "        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
        "        total_loss = reconstruction_loss + kl_loss\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\"loss\": self.total_loss_tracker.result(), \"reconstruction_loss\": self.reconstruction_loss_tracker.result(), \"kl_loss\": self.kl_loss_tracker.result()}\n",
        "\n",
        "\n",
        "### 5. TREINAMENTO DO MODELO (COM LÓGICA DE SALVAR/CARREGAR)\n",
        "vae = VAE(encoder, decoder)\n",
        "# Compilamos com loss=None porque a perda já é calculada dentro do train_step\n",
        "vae.compile(optimizer=keras.optimizers.Adam(), loss=None)\n",
        "\n",
        "# --- LÓGICA DE CARREGAMENTO --- # <-- ADICIONADO\n",
        "# Verifica se já existe um arquivo de pesos salvo\n",
        "if os.path.exists(WEIGHTS_FILE):\n",
        "    print(f\"\\n--- Arquivo de pesos '{WEIGHTS_FILE}' encontrado! Carregando conhecimento... ---\")\n",
        "    # Carrega os pesos no modelo antes de treinar\n",
        "    # O build_shape é necessário para que o modelo saiba a estrutura dos pesos a carregar\n",
        "    vae.build(input_shape=(None, ORIGINAL_DIM))\n",
        "    vae.load_weights(WEIGHTS_FILE)\n",
        "else:\n",
        "    print(f\"\\n--- Nenhum arquivo de pesos encontrado. Iniciando treinamento do zero. ---\")\n",
        "\n",
        "# Treina o modelo (seja do zero ou continuando)\n",
        "print(f\"--- Treinando por mais {EPOCHS} épocas... ---\")\n",
        "vae.fit(x_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=x_test)\n",
        "\n",
        "# --- LÓGICA DE SALVAMENTO --- # <-- ADICIONADO\n",
        "# Salva os pesos atualizados no arquivo após o treinamento\n",
        "print(f\"\\n--- Salvando os pesos atualizados em '{WEIGHTS_FILE}'... ---\")\n",
        "vae.build(input_shape=(None, ORIGINAL_DIM)) # Build the model before saving\n",
        "vae.save_weights(WEIGHTS_FILE)\n",
        "print(\"--- Pesos salvos com sucesso! ---\")\n",
        "\n",
        "\n",
        "### 6. DEMONSTRAÇÃO DA COMPRESSÃO E DESCOMPRESSÃO (Seu código, sem alterações)\n",
        "def plot_results(models, data, batch_size=128, model_name=\"vae_mnist\"):\n",
        "    \"\"\"Função para visualizar os resultados.\"\"\"\n",
        "    encoder, decoder = models\n",
        "    x_test, y_test = data\n",
        "    print(\"\\n--- DEMONSTRAÇÃO DA COMPRESSÃO SEMÂNTICA ---\")\n",
        "    print(f\"Dimensão Original de 1 imagem: {x_test[0].shape[0]} números (pixels)\")\n",
        "    z_mean, _, _ = encoder.predict(x_test, batch_size=batch_size)\n",
        "    print(f\"Dimensão Comprimida (Vetor Latente) de 1 imagem: {z_mean[0].shape[0]} números\")\n",
        "    taxa_compressao = 1 - (LATENT_DIM / ORIGINAL_DIM)\n",
        "    print(f\"Taxa de Compressão: {taxa_compressao:.2%}\")\n",
        "    print(\"\\nO vetor abaixo é o 'arquivo comprimido' da primeira imagem de teste:\")\n",
        "    print(z_mean[0])\n",
        "    reconstructed_imgs = decoder.predict(z_mean, batch_size=batch_size)\n",
        "\n",
        "    n = 10\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for i in range(n):\n",
        "        ax = plt.subplot(2, n, i + 1)\n",
        "        plt.imshow(x_test[i].reshape(28, 28))\n",
        "        plt.title(\"Original\")\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "        ax = plt.subplot(2, n, i + 1 + n)\n",
        "        plt.imshow(reconstructed_imgs[i].reshape(28, 28))\n",
        "        plt.title(\"Reconstruída\")\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "    plt.suptitle(\"Resultados da Compressão Semântica\", fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "plot_results(models=(encoder, decoder), data=(x_test, y_test), batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nova seção"
      ],
      "metadata": {
        "id": "QYEWDoCfyLJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uBbfa2oKwlq8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}